{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_housing_data():\n    DATA_PATH = '../input/hands-on-machine-learning-housing-dataset/housing.csv'\n    data = pd.read_csv(DATA_PATH)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = load_housing_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Studying the data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a test set, 1st way:\n**This way of spliting test and training set is not good, because every time I run the code, the test set changes, that is, there are instances in the new test set that were previously in the training set. Doing things this ways makes the test set corrupted, poluted with the training one.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data)) #returns a shuffled numpy array\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\n##so the np.random.permutation shuffles the order of the rows in a DataFrame and returns an np array    \n#df_test = pd.DataFrame({'column_1':[1,2,3,4], 'column_2':[5,6,7,8]})\n#print(df_test)\n#np.random.permutation(df_test)\n#np.random.permutation(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A few new things:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(np.random.permutation)\n#help(pd.DataFrame.iloc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using the split_train_test() function:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = split_train_test(housing, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a test set, 2nd way:\n**This way the test set won't contain instances that have been in the train set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from zlib import crc32\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set] # ~ is used to compare binary numbers\n\n## The lambda keyword is used to create small anonymous functions.\n## A lambda function can take any number of arguments, but can only have one expression.\n## The expression is evaluated and the result is returned.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**A few new things**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(pd.DataFrame.loc) # Access a group of rows and columns by label(s) or a boolean array.\n#help(pd.DataFrame.apply) #apply a function along the axis of a DataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"creatind a new dataframe, but with an id:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_with_id = housing.reset_index() #adds an 'index' column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')\nhousing_with_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a unique caracterisc for each row, 3rd way:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**But those indices are not that unique, for exmple if the data gets changed. One must choose a better unique characteristc, for exemple longitude together with latitude wont change. A better aproach is try to combine them.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_with_id[\"id\"] = housing[\"longitude\"]*1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\nhousing_with_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now with scikit-learn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nprint(len(train_set), \"/\",len(test_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# creating strata for \"median_income\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function below converts bin values into discrete intervals\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6.0, np.inf],\n                               labels=[1, 2, 3, 4, 5])\nhousing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.inf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(pd.cut)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The stratified `train` and `test` set are the official ones\nNow that i stratified the median income, i'm ready to do stratified sampling based on the income cathegory. \"StratifiedSuffleSplit\" sklearn' class will help:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below, we can see that the stratification was successfull","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set['income_cat'].value_counts() / len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['income_cat'].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, removing 'income_cat' attribute so the data is back to its original state:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(labels=[\"income_cat\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the data to gain insights","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Create a copy for exploration, so that i can play with it without harming the training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train_set = strat_train_set.copy() ###explore only the TRAIN set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###`alpha` creates a better visualization, wich highlights high density areas\nexp_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below a pica-mega-blaster vizualization !!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n                   s=exp_train_set['population']/100, label='population', figsize=(10,7),\n                   c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#help(exp_train_set.plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking For Correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = exp_train_set.corr()##returns a DataFrame\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n##below are the most promissing attributes\nattributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\nscatter_matrix(exp_train_set[attributes], figsize=(12,8))## thiss is a pandas function","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most promissing attribute is  the `median_income`, so i'll zoom in on their correlation scatterplot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train_set.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experimenting with attribute Combinations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train_set['rooms_per_household'] = exp_train_set['total_rooms'] / exp_train_set['households']\nexp_train_set['bedrooms_per_rooms'] = exp_train_set['total_bedrooms'] / exp_train_set['total_rooms']\nexp_train_set['population_per_household'] = exp_train_set['population'] / exp_train_set['households']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the new correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = exp_train_set.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the Data for Machine Learning Algorithms","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Data Cleaning:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## drop() creates a copy of the data and does not affect strat_train_set\nprep_train_set = strat_train_set.drop('median_house_value', axis=1)\nprep_train_set_labels = strat_train_set['median_house_value'].copy()\nprep_train_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now I have 3 options: **\n1. Get rid of corresponding districts;\n2. Get rid of the whole attribute;\n3. Set the values to some value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Option 1:\n# prep_train_set.dropna(subset=[\"total_bedrooms\"])\n#\n# Option 2:\n# prep_train_set.drop(total_bedrooms, axis=1)\n#\n# Option 3:\n# median = prep_train_set['total_bedrooms'].median()\n# prep_train_set['total_bedrooms'].fillna(median, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But sklearn provides a handy class to take care of missing values: `SimpleImputer`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\n\n## the line below is necessary because we have to have only numeric values\nprep_train_set_num = prep_train_set.drop('ocean_proximity', axis=1)\n\nimputer.fit(prep_train_set_num)## THIS IS A \"TRAINNED\" IMPUTER\n## imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable\nimputer.statistics_\n\n## now i can use this \"trained\" imputer to transform the training set by replacing missing values with the learned medians:\nX = imputer.transform(prep_train_set_num)## The result is a plain numpy array containing the transformed features.\n                                         ## If you want tto put it back into a pandas DataFrame, it's simple:\n\nprep_train_set_tr = pd.DataFrame(X, columns=prep_train_set_num.columns,\n                                 index=prep_train_set_num.index)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_train_set_num.median().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_train_set_tr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.strategy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prep_train_set_cat = strat_train_set['ocean_proximity'] ## this line creates a Series\nprep_train_set_cat = strat_train_set[['ocean_proximity']] ## this one creates a DataFrame\nprep_train_set_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(prep_train_set_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`ocean_proximity` is a categorical attribute. But most machine learning algorithms prefer to work with numbers, so let's converts these categories from text to numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nprep_train_set_cat_encoded = ordinal_encoder.fit_transform(prep_train_set_cat)\nprep_train_set_cat_encoded[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scikit_learn provides a `OneHotEncoder` class to convert categorical values into one-hot vectors:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nprep_train_set_cat_1hot = cat_encoder.fit_transform(prep_train_set_cat)\nprep_train_set_cat_1hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_train_set_cat_1hot.toarray() ## this is a scipy module","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# help(prep_train_set_cat_1hot.toarray)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Transformers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### THE CODE IN THIS CELL I DID NOT UNDERSTAND VERY WELL\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n        population_per_household = X[:, population_ix] / X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nprep_train_set_extra_attribs = attr_adder.transform(prep_train_set.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pipeline:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\nprep_train_set_num_transformed = num_pipeline.fit_transform(prep_train_set_num)\n\n########################## mass transformation\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = list(prep_train_set_num)\ncat_attribs = ['ocean_proximity']\n\nfull_pipeline = ColumnTransformer([## this is the full pipeline for the data transformation\n    ('num', num_pipeline, num_attribs),\n    ('cat', OneHotEncoder(), cat_attribs)\n])\n\nprep_train_set_prepared = full_pipeline.fit_transform(prep_train_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting and training a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_train_set_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prep_train_set_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing LinearRegression model:**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At this step i'm testing on the training set, this is a good way of evaluationg overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared, housing_labels = prep_train_set_prepared, prep_train_set_labels\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions: ', lin_reg.predict(some_data_prepared))\nprint('Labels:', list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nprint(\"Linear, Root Mean Squared Error:\", lin_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The linear regression model underfitted the data, so i'll try a more powerful model, `DecisionTreeRegressor`:**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Testing DecisionTreeRegressor model:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\nprint(\"Tree, Root Mean Squared Error:\", tree_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Error with the `DecisionTreeRegressor` is zero, that means that the model overfitted the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Better Evaluation Using Cross-Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ntree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10) ## 10 folds\ntree_rmse_scores = np.sqrt(-tree_scores)\ntree_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Standard Deviation:', scores.std())\n\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So `DecisionTreeRegressor` did not performe that well, now doing the same cross validation for `LinearRegression`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's right: the Decision Tree model is overfitting so badly that it performs worse than the linear regression model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Testing the RandomForestRegressor model:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nprint(\"Tree, Root Mean Squared Error:\", forest_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now some cross-validation with `RandomForestRegressor`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10) ## 10 folds\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the Random Forest performed much better than the previous models, but is overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine-tunning the model\nLet's assume that now I have a shorlist of promissing models. I now need to fine tune them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Grid Search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\n\ngrid_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Printing each one of the combination results:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_rooms']\ncat_encoder = full_pipeline.named_transformers_['cat']\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Congratulations to me, i've successfully fine-tuned my best model!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the system on the Test Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop('median_house_value', axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is. For this, you can compute a 95% confidence interval for the generalization error using `scipy.stats.t.interval()`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}